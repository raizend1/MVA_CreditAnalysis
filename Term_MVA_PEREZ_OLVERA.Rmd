---
title: "MVA Final Project"
author:
- name: Francisco Perez
- name: Alejandro Olvera
date: "June 1st 2017"
output:
  pdf_document:
    fig_caption: yes
    latex_engine: xelatex
  html_document: default
  word_document: default
graphics: yes
bibliography: bibliography.bib
---

```{r setup, include=FALSE,comment=NA}
#***************************************************************************#
#                           0. Initialization                               #
#***************************************************************************#

# Initialise workspace, remove old objects for safety resons and define a utility function
rm(list=ls(all=TRUE))
set.seed(123)
source("Term_MVA_PEREZ_OLVERA_utility_functions.R")
source("workingDir.R")

setwd(codeDir)

# Required libraries
library(ggplot2)
library(mice)
library(kernlab)
library(class)
library(e1071)
library(psych)
library(DMwR)
library(chemometrics)
library(ggrepel)
library(ggthemes)
library(robustbase)
library(knitcitations)
library(doParallel)
```


## Motivation
Datasets came from a wide variety of sources: medical, ecological, economics, social welfare and status, etc. This data are inherently complex and it is common to find that just a single response variable does not describe the behaviour of the entire system. Multivariate analysis deals with these scenarios, pocessing techniques that can reveal hidden information, otherwise undetected by univariate methods. Generally, multivariate approaches are favoured to multiple executions of univariate methods as they save time and conserve statistical power which is quickly lost through multiple testing. 

The objective of this work, is to apply the multivariate techniques learn in classes, to discover new features and the relation of the variables in a given dataset.

## Description of the dataset
For this analysis, we will use the default of credit card clients Data Set from UCI Machine Learning Repository [@defaultDataSet] that contains the data from customers default payments in Taiwan from the year 2005. 

The data contains one response variable and 23 explanatory variables, defined as follows:

* X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.

* X2: Gender (1 = male; 2 = female).

* X3: Education (1 = graduate school; 2 = university; 3 = high school; 0, 4, 5, 6 = others).

* X4: Marital status (1 = married; 2 = single; 3 = divorce; 0=others).

* X5: Age (year).

* X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows:
  + X6 = the repayment status in September, 2005; 
  + X7 = the repayment status in August, 2005; . . .;
  + X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: 

  + -2: No consumption; 
  + -1: Paid in full; 
  + 0: The use of revolving credit; 
  + 1 = payment delay for one month; 
  + 2 = payment delay for two months; . . .; 
  + 8 = payment delay for eight months; 
  + 9 = payment delay for nine months and above.

* X12-X17: Amount of bill statement (NT dollar). 
  + X12 = amount of bill statement in September, 2005; 
  + X13 = amount of bill statement in August, 2005; . . .; 
  + X17 = amount of bill statement in April, 2005. 

* X18-X23: Amount of previous payment (NT dollar). 
  + X18 = amount paid in September, 2005; 
  + X19 = amount paid in August, 2005;...
  + X23 = amount paid in April, 2005.


* Y: client's behavior; 
  + Y=0 then not default
  + Y=1 then default
  
## Preprocessing 
### Methodology

The selected language to do the processing is R, with its IDE R Studio. For the plots we will use ggplot and lattice.
The first step is to import the data, and to have some insight of what we are dealing with, we will describe the dimensions and the attributes of the dataset, from now on called 'credit'. What we have is:

### Data Loading and description 
```{r Data loading, include=FALSE}
# Read initial data
data.path <- glue(dataDir,"/","default_of_credit_card_clients.csv")
credit <- read.table(data.path, header = TRUE,sep = ";")
```

```{r Data first output, include=FALSE,comment=NA}
dim(credit)
summary(credit)
str(credit)
```
Thus we now know that this is a dataset with 30000 rows and 25 variables. All variables are defined as continuous integers, but this is not correct according to the dataset creators' description, so we need to change *EDUCATION*, *MARRIAGE*, the payments from *PAY_0* to *PAY_6* and the response variable *default.payment.next.month* to categorical. Also we dont need the column *ID*, so we removed it.
TODO: document limit bill

```{r Change class of variables, include=FALSE}
# Change 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE' and 'default.payment.next.month' to categorical
factor.indexes <- which(names(credit) %in% c("PAY_0","PAY_2","PAY_3","PAY_4","PAY_5","PAY_6","SEX","EDUCATION","MARRIAGE","default.payment.next.month")) 
credit[,factor.indexes] <- lapply(credit[,factor.indexes], as.factor)

# Remove unnecesary data: ID
credit<- credit[,-1]
factor.indexes<-factor.indexes-1 # update indexes of the factors

# Rename the levels of the categorical values for better unsderstanding
levels(credit$SEX) <- c("Male", "Female")
levels(credit$EDUCATION) <- c("Uk1", "Grad.", "Univ.", "H.School", "Uk2", "Uk3", "Uk4")
levels(credit$MARRIAGE) <- c("Other", "Married", "Single", "Divorced")
levels(credit$default.payment.next.month) <- c("Not default", "Default")
# rename factor variables from columns PAY 6 to 11
for(i in 6:11){
  # levels(credit[,i]) <- c("No consumption", "Paid in full","Use of revolving credit","Payment delay 1M","Payment delay 2M",
  #                         "Payment delay 3M","Payment delay 4M","Payment delay 5M","Payment delay 6M","Payment delay 7M",
  #                         "Payment delay 8M")
  levels(credit[,i]) <- c("NC", "PF","URC","PD1","PD2",
                          "PD3","PD4","PD5","PD6","PD7","PD8")
}
```

### Check Zero variance predictors 
Are there any zero variance predictors? nearZeroVar() diagnoses predictors that have one unique value (i.e. are zero variance predictors) or predictors that are have both of the following characteristics: they have very few unique values relative to the number of samples and the ratio of the frequency of the most common value to the frequency of the second most common value is large. 
```{r Check Zero Values, echo=FALSE,include=FALSE}
 
library("caret")
cl <- makeCluster(detectCores())
registerDoParallel(cl)
zero.variance <- nearZeroVar(credit, saveMetrics = TRUE)
stopCluster(cl)
str(zero.variance)
zero.variance[zero.variance[,"zeroVar"] > 0, ] 
zero.variance[zero.variance[,"zeroVar"] + zero.variance[,"nzv"] > 0, ] 
```

There are none, we can conclude that all the predictors are relevant for the moment.

### Check missing Values 
TODO: document the observations and criteria to exclude the observations

```{r Check missing values, echo=FALSE}
# First check N/A values
which(is.na(credit),arr.ind=TRUE) 
md.pattern(credit)
# It can't find any value expressed as 'NA', but there are some rows where all the values for the billing
# statements and the previous payment are 0, this could be treated as a missing value, because if there is 
# a credit card issued, there must be values for this columns, so we treat them as missing values. First 
# a check of how many of this occurrences exist is needed
check.zero.rows<-function(input.data){
  indexes<-NULL
  j<-1
  for(i in 1:dim(input.data)[1]){
    if((!all(input.data[i,c(1:6)]=="NC")) && all(input.data[i,c(7:dim(input.data)[2])]==0)){
      indexes[j]<- i
      j<-j+1
    }
  }
  return(indexes)
}

cl <- makeCluster(detectCores())
registerDoParallel(cl)
num.zeros.index<-check.zero.rows(credit[,c(6:23)])
stopCluster(cl)
length(num.zeros.index)
round((length(num.zeros.index)*100)/dim(credit)[1],2)

credit<-credit[-num.zeros.index,]

# update continuous and factor data
credit.continuos<-credit[,-factor.indexes]
credit.factors<-credit[,factor.indexes]

```
So in total there are 495 of this kind of data, it represents 1.68% of the data, but we will discard them anyway because this data can produce wrong calculations.

## Check distribution of Data  
Let's check the distribution of all the variables. For the continuous ones we can plot an histogram, for the categorical ones, a barplot with the distribution within the levels of the variable.
```{r fig1,fig.width=18,fig.height=18,message=TRUE,fig.cap="Variable distribution", echo=TRUE}
#knitr::opts_chunk$set(cache=TRUE)
setwd(plotDir)
jpeg("Variable_Distribution.jpeg",width = 2000,height = 900,units = "px",res = 150)
grid.plot(credit,15)
dev.off()
setwd(codeDir)
```
with this plot, we can see that the continuous data is very skewed, and not normal at all, we will apply some transformations to make the data more "normal"

## Outlier Detection   

```{r Local Oulier Factor, echo=FALSE}
#***************************************************************************#
#                            2.4 Outlier Detection                          #
#***************************************************************************#
knitr::opts_chunk$set(cache=TRUE)
# #**************************** Outlier detection with lofactor (Local Outlier Factor) ***********************************
# #outlier detection with lofactor (Local Outlier Factor), takes a while 
# require(DMwR)
# outlier.scores <- lofactor(credit[,-factor.indexes], k=10)
# #we cannot plot, there are NaN, infinite values, possible cause is to have more repeated values than neighbours k
# plot(density(outlier.scores))
# #pick top 5 as outliers
# outliers <- order(outlier.scores, decreasing=T)[1:5]
# hist(outliers)
# #Which are the outliers?
# print(outliers)
# # we create a table of scores and id, to remove the supossed outliers
# scores <- cbind.data.frame(score = outlier.scores,id = rownames(credit.continuos))
# credit.lofactor <- credit[-as.numeric(scores[scores$score >= scores[outliers[5],]$score,]$id)]
# ```
# 
# ```{r, fig2,fig.width=8,fig.height=4,message=FALSE,fig.cap="Caption", echo=FALSE}

#**************************** Outlier detection Mahalanobis ***********************************
#outlier detection with mahalanobis 
require(chemometrics)
outlier.scores <- Moutlier(credit[,-factor.indexes],quantile=0.975,plot=TRUE)
credit <- subset(credit,outlier.scores$md<outlier.scores$cutoff)

# update of continuous and categorical subsets
credit.continuos<-credit[,-factor.indexes]
credit.factors<-credit[,factor.indexes]

#cleaning up the house
remove(outlier.scores)

# Altought, from all this, in some cases, the're could be just rich people in some variables, 
# or really indebted people in others
```

## Detection of most correlated continuous variables 

```{r, echo=FALSE}
require(corrplot)
par(mfrow=c(2,2))
corrplot(cor(credit.continuos), method="circle")
corrplot(cor(credit.continuos), method="number")
par(mfrow=c(1,1))
```
From the correlation calculus, we can see that there is a clear relationship between the values of BILL_AMT(x) and BILL_AMT(x+1), we will check this with PCA, to observe if there is a dimension that it is not obviuos to capture and to decide wich variables could be added.

## Detection of (correlation) of categorical variables 
In this part we are going to analyze each one of the categorial variables with respect to the response variable, if there is anything interesting that gives us more information.

### Sex 
```{r, echo=FALSE}
# How many males and females do we have?
initial.barplot(credit,SEX)
# We have 34 % more females than males in our dataset.

# How many Default's and Not-Defaults's do we have for each sex?
grouped.count.plot(credit,SEX,default.payment.next.month)

# It seems that females tend to have less default payments, 
# lets compute the exact proportion to see if there is some kind of bias.
freq.table <- (with(data = credit, table(SEX, default.payment.next.month)))
p.table <- round(prop.table(freq.table, margin = 1), digits = 3)
cbind(freq.table, p.table)
# As we see, the proportion of males with default payment is 0.242, and the proportion of females is 0.208.
# Indeed, males in general have a higher tendency of default payment.
```

### Education

```{r, echo=FALSE}
# a count of all the values to get an initial idea
initial.barplot(credit,EDUCATION)
# we can see that the 4 "unknown" values are very few, comparing them with the others
# also university is most present in this data, so a better way to see this is to group them all.

# a count check of all the education respect to default payment
grouped.count.plot(credit,EDUCATION,default.payment.next.month)
# university has the most population in both cases, but the tendency is to be not default,
# so a prior assumption will be that university level koreans will be unable to fill their
# debt obligations on time

# Again a check of the proportions will be useful
freq.table <- table(credit$EDUCATION, credit$default.payment.next.month)
p.table <- round(prop.table(freq.table, margin = 1), digits = 3)
cbind(freq.table, p.table)
# The data have interesting results, first showing that not default are the most likely case in
# the unknown categories, and also showing that even if university is the bigest tendency,
# graduate level koreans are the ones that in proportion tend to be unable to fullfill their
# debt obligations in time
```

### Marriage

```{r, echo=FALSE}
# How are the levels of the variable distributed?
initial.barplot(credit,MARRIAGE)

# Basically we have 'Married' and 'Single' individuals, here we have the percentages of each type
round(prop.table(table(credit$MARRIAGE)) * 100, digits = 1)

# How many Default's and Not-Defaults's do we have for each type of marriage?
grouped.count.plot(credit,MARRIAGE,default.payment.next.month)

# Let's compute the exact proportion for each level to see if there is some kind of bias.
freq.table <- (with(data = credit, table(MARRIAGE, default.payment.next.month)))
p.table <- round(prop.table(freq.table, margin = 1), digits = 3)
cbind(freq.table, p.table)

# As we can see, the proportion of 'Married' individuals with default payement is 0.235, while the proportion of 
# 'Single' is 0.209. 'Married' individuals have a higher tendency of default payement. 'Other' has a very low percentage 
# of default payment, but we just have 54 individuals, which is not enough data. 'Divorced' has the higher 
# percentage of default, but again we just have 323 individuals, compared to the +20000 rows that 
# are either 'Married' or 'Single'.
```

## Age

```{r, echo=FALSE}
# Even AGE is not categorical, we wanted to do an analysis to check how the age are related to the 
# default or not default category
# a count of all the values to get an initial idea
initial.barplot(credit,AGE)

require(plyr)
head(arrange(as.data.frame(table(credit$AGE)),desc(Freq)), n = 5)
# from this analysis, it is obvious that the quantity of users of credit cards, are centered around
# 29 years old

# Again a check of the proportions will be useful
freq.table <- table(credit$AGE, credit$default.payment.next.month)
p.table <- round(prop.table(freq.table, margin = 1), digits = 3)
(age.df<-as.data.frame(cbind(age=row.names(p.table),freq.table, p.table)))
head(arrange(age.df,desc(age.df$`Not default`)), n = 5)
# in this case, the proportions for not default are around 31 years, somewhat closed from the total
# around 29, but for 57 years, there is also a higher ratio in here, the default parameter is dominant
# in all the cases
head(arrange(age.df,desc(age.df$Default)), n = 5)
# this ratio is somewhat scattered, but they are around 51 and 64 years old, the most default is 
# predominant.
```

## PCA Construction

```{r, include=FALSE}
n<-nrow(credit.continuos)
p<-ncol(credit.continuos)

#define matrix N
weight <- rep(1,n)
N <- diag(weight/sum(weight))

#calculate centroids
G<-t(as.matrix(credit.continuos)) %*% N %*% rep(1,n)

#centering X
X <-scale(credit.continuos,scale = FALSE,center = G)

#covariance matrix
covX <- t(X) %*% N %*% X

#standardizing
Xs<-scale(X)
Xs <- Xs * sqrt(n)/sqrt(n-1)

#get eigenvalues 
eigX <- eigen(covX)

plot(eigX$value,type="l",main="Screeplot")
cumsum(100*eigX$values/sum(eigX$values))
require(FactoMineR)
e_ncp<-estim_ncp(X, ncp.min=0, ncp.max=p-1, scale=TRUE, method="GCV")
ncp<-e_ncp$ncp

#cleaning up the house
rm(N,G,X,Xs,covX,eigX)
gc()
```

# ```{r, echo=FALSE}
# require(FactoMineR)
# #par(mfrow=c(2,2))
# credit.continuos.Moutlier$default.payment.next.month <- credit.Moutlier$default.payment.next.month
# credit.PCA1 <- PCA(credit.continuos.Moutlier,quali.sup = c(which(names(credit.continuos.Moutlier)==c("default.payment.next.month"))),ncp = e_ncp$ncp)
# ```

```{r, echo=FALSE}
credit.continuos$default.payment.next.month <- credit$default.payment.next.month
credit.PCA <- PCA(credit.continuos,quali.sup = c(which(names(credit.continuos)==c("default.payment.next.month"))),ncp=ncp)
summary(credit.PCA)
```

```{r, echo=FALSE}
require(factoextra)
#PCA Colored according to the quality of representation of the variables
fviz_pca_var(credit.PCA, col.var="cos2") +
scale_color_gradient2(low="white", mid="blue", high="red", midpoint=0.5) + theme_minimal()
# If a variable is perfectly represented by only two components, the sum of the cos2 is equal to one. In this case the variables will be positioned on the circle of correlations.
# For some of the variables, more than 2 components are required to perfectly represent the data. In this case the variables are positioned inside the circle of correlations.
```

```{r, echo=FALSE}
# Coordinates of variables
head(credit.PCA$var$coord)
head(credit.PCA$var$contrib)
#All the BILL_AMT are well represented and contribute the most to dim 1. LIMIT_BALL is more related to dim2
dimdesc(credit.PCA)
```

```{r, echo=FALSE}
#Plot the quality of the representation for individuals on the principal components and colour them using the cos2 values 
fviz_pca_ind(credit.PCA, col.ind="cos2") +
scale_color_gradient2(low="white", mid="blue", 
   high="red", midpoint=0.50) + theme_minimal()
```

```{r, echo=FALSE}
credit.factors$default.payment.next.month <- credit.factors$default.payment.next.month
credit.MCA<-MCA(credit.factors, quali.sup = c(which(names(credit.factors)==c("default.payment.next.month"))),level.ventil = 0.2,ncp=20)
```

```{r, echo=FALSE}
dimdesc(credit.MCA, axes=1:2, proba=0.05)
# Visualize top 10 variable categories
fviz_mca_var(credit.MCA, select.var = list(cos2 = 10))
```

```{r, include=TRUE}
#*****************************************************************************************#
#                               Definition of clusters                                    #
#*****************************************************************************************#
# Number of clusters
#library(rpud)  
#credit.hcpc <- HCPC(credit.MCA)
#credit.hcpc <- rpuHclust(credit.MCA)
#cleaning up the house
#rm(credit.hcpc)
#gc()
```


```{r, include=TRUE}
#*****************************************************************************************#
#                                  2.7 Feature Extraction                                #
#****************************************************************************************#
```

```{r, include=FALSE}


```


We will create a PCA for all the continuos variables and a MCA for the categorical ones, determining the relation that all the variables has to each other. In this exploratory analyzis, we first check the balance of the sample with respect to our response variable:
```{r, echo=FALSE,eval=TRUE,comment=NA}
# We first check if the test and train samples are balanced
rbind(noquote(table(credit$default.payment.next.month)),sapply(prop.table(table(credit$default.payment.next.month))*100, function(u) noquote(sprintf('%.2f%%',u))))
```
So we first deal with this, using 

```{r, include=TRUE}
#*****************************************************************************************#
#                              Initial model assumptions                                  #
#*****************************************************************************************#
# Checking PCA


#credit.glm<-glm()

#prediction directly with log
require(caret)
# NNModel2 <- train(Train[,-10], trainResponse,
#                   method = "nnet",
#                   preProcess = c("pca"),
#                   trControl= trainControl(method = "cv", number = 10),
#                   tuneGrid = expand.grid(.size=c(1,5,10, 15, 20, 25, 30, 35, 40),.decay=c(0,0.001,0.1, 0.2, 0.3, 0.4, 0.5, 1, 2, 3, 4)))

n.rows <- nrow(credit)
n.cols <- ncol(credit)

#initial linear model -> this must be reduced either by factor or dimensionality
credit.lm<-lm(default.payment.next.month~.,data = credit)
# plot(credit[,-10],credit[,10])
# abline(credit.lm)

# get just one third for validation, the rest to train
test.indexes <- sample(1:n.rows,size = floor(n.rows*0.3),replace = FALSE)
credit.test <- credit[test.indexes,]
credit.train <- credit[-test.indexes,]

# method to do cross validation for tunning

#array for the best parameters
# c.best <- c()
# epsilon.best <- c()
# gamma.best<-c()
# polynomial.degree.best<-c()

#array for computation time
# compu.time<- c()
# 
# # use svm
# model2 <- svm(credit.train[,-25],credit.train[,25],epsilon=0.01,gamma=200, C=100)
# lines(credit.train[,-25],predict(model2,credit.train[,-25]),col="green")
# credit.svm<-ksvm(credit.train[,-25],credit.train[,25],epsilon=0.01, C=100)
# 
# 
# library(rpart)
# library(rpart.plot)
# library(rattle)
# p2 = rpart(default.payment.next.month ~ ., data=credit, control=rpart.control(cp=0.001, xval=10))
# p2
# plot(p2)
# 
# # THE SEQUENCE OF TREES WITH THEIR COMPLEXITY PARAMETER AND COMPUTED ERROR IN THE TRAINING SAMPLE AND BY CROSSVALIDATION
# printcp(p2)
# 
# plot(p2$cptable[,2],p2$cptable[,3],type="l",xlab="size of the tree",ylab="Relative impurity",main="R(t)")
# lines(p2$cptable[,2],p2$cptable[,4],col="blue")
# legend("topright",c("R(T)training","R(T)cv"),col=c("black","blue"),lty=1)
# plotcp(p2)

```

```{r generateBibliography, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
require("knitcitations")
cleanbib()
options("citation_format" = "pandoc")
read.bibtex(file = "bibliography.bib")
```