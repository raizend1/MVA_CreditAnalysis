---
title: "MVA Final Project"
author:
- name: Francisco Perez
- name: Alejandro Olvera
date: "June 7th 2017"
header-includes: 
  - \floatsetup[table]{capposition=bottom}
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    toc: yes
    toc_depth: 2
  html_document: default
  word_document: default
graphics: yes
bibliography: bibliography.bib
---
\newpage
```{r setup, include=FALSE,comment=NA}
#***************************************************************************#
#                           0. Initialization                               #
#***************************************************************************#

# Initialise workspace, remove old objects for safety resons and define a utility function
rm(list=ls(all=TRUE))
set.seed(123)
source("Term_MVA_PEREZ_OLVERA_utility_functions.R")
source("workingDir.R")

setwd(codeDir)

# Required libraries
library(ggplot2)
library(mice)
library(kernlab)
library(class)
library(e1071)
library(psych)
library(DMwR)
library(chemometrics)
library(ggrepel)
library(ggthemes)
library(robustbase)
library(knitcitations)
library(doParallel)
```


## Motivation
Datasets came from a wide variety of sources: medical, ecological, economics, social welfare and status, etc. This data are inherently complex and it is common to find that just a single response variable does not describe the behaviour of the entire system. Multivariate analysis deals with these scenarios, pocessing techniques that can reveal hidden information, otherwise undetected by univariate methods. Generally, multivariate approaches are favoured to multiple executions of univariate methods as they save time and conserve statistical power which is quickly lost through multiple testing. 

The objective of this work, is to apply the multivariate techniques learn in classes, to discover new features and the relation of the variables in a given dataset.

## Methodology
The selected language to do the processing is R, with its IDE R Studio. For the plots we will use ggplot and lattice. The methodology is sequential, as seen in Multivariate Analysis classes

## Description of the dataset
For this analysis, we will use the default of credit card clients Data Set from UCI Machine Learning Repository [@defaultDataSet] that contains the data from customers default payments in Taiwan from the year 2005. 

The data contains one response variable and 23 explanatory variables, defined as follows:

* X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.

* X2: Gender (1 = male; 2 = female).

* X3: Education (1 = graduate school; 2 = university; 3 = high school; 0, 4, 5, 6 = others).

* X4: Marital status (1 = married; 2 = single; 3 = divorce; 0=others).

* X5: Age (year).

* X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows:
  + X6 = the repayment status in September, 2005; 
  + X7 = the repayment status in August, 2005; . . .;
  + X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: 

  + -2: No consumption; 
  + -1: Paid in full; 
  + 0: The use of revolving credit; 
  + 1 = payment delay for one month; 
  + 2 = payment delay for two months; . . .; 
  + 8 = payment delay for eight months; 
  + 9 = payment delay for nine months and above.

* X12-X17: Amount of bill statement (NT dollar). 
  + X12 = amount of bill statement in September, 2005; 
  + X13 = amount of bill statement in August, 2005; . . .; 
  + X17 = amount of bill statement in April, 2005. 

* X18-X23: Amount of previous payment (NT dollar). 
  + X18 = amount paid in September, 2005; 
  + X19 = amount paid in August, 2005;...
  + X23 = amount paid in April, 2005.


* Y: client's behavior; 
  + Y=0 then not default
  + Y=1 then default
  
## Preprocessing 
The first step is to import the data, and to have some insight of what we are dealing with, we will describe the dimensions and the attributes of the dataset, from now on called 'credit'. What we have is:

### Data Loading and description 
We will read the dataset into the *credit* variable, from now on we will work on that dataset. In a first inspection of its dimensions, summary and structure, we can get that this is a dataset with 30000 rows and 25 variables. All variables are defined as continuous integers, but this is not correct according to the dataset creators' description, so we need to change *EDUCATION*, *MARRIAGE*, the payments from *PAY_0* to *PAY_6* and the response variable *default.payment.next.month* to categorical. Also we dont need the column *ID*, so we removed it.
Also for a better visualization and identification of the *PAY* predictors, we have decided to rename all the levels according to the values indicated in Table 1. FOr the values on payment delay,*'X'* describes the amount of months for each value.
```{r Data loading, include=FALSE}
# Read initial data
data.path <- glue(dataDir,"/","default_of_credit_card_clients.csv")
credit <- read.table(data.path, header = TRUE,sep = ";")
```

```{r Data first output, echo=FALSE,comment=NA}
dim(credit)
summary(credit)
str(credit)
```

```{r Change class of variables, include=FALSE}
# Change 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE' and 'default.payment.next.month' to categorical
factor.indexes <- which(names(credit) %in% c("PAY_0","PAY_2","PAY_3","PAY_4","PAY_5","PAY_6","SEX","EDUCATION","MARRIAGE","default.payment.next.month")) 
credit[,factor.indexes] <- lapply(credit[,factor.indexes], as.factor)

# Remove unnecesary data: ID
credit<- credit[,-1]
factor.indexes<-factor.indexes-1 # update indexes of the factors

# Rename the levels of the categorical values for better unsderstanding
levels(credit$SEX) <- c("Male", "Female")
levels(credit$EDUCATION) <- c("Uk1", "Grad.", "Univ.", "H.School", "Uk2", "Uk3", "Uk4")
levels(credit$MARRIAGE) <- c("Other", "Married", "Single", "Divorced")
levels(credit$default.payment.next.month) <- c("Not default", "Default")
# rename factor variables from columns PAY 6 to 11
for(i in 6:11){
  # levels(credit[,i]) <- c("No consumption", "Paid in full","Use of revolving credit","Payment delay 1M","Payment delay 2M",
  #                         "Payment delay 3M","Payment delay 4M","Payment delay 5M","Payment delay 6M","Payment delay 7M",
  #                         "Payment delay 8M")
  levels(credit[,i]) <- c("NC", "PF","URC","PD1","PD2",
                          "PD3","PD4","PD5","PD6","PD7","PD8")
}

#Save the id's of each individual to a variable
iden = rownames(credit)

```

`r require(xtable)
xtable(
  data.frame(
    'Original Value' = c('-2', '-1', '0', '1 to 8'),
    Description = c('No consumption', 'Paid in full', 'The use of revolving credit','Payment delay for (X) months'),
    'Rename as' = c('NC', 'PF', 'URC','PD(X)')),
  caption = 'Codification of PAY categorical values')`

| Original Value | Description                  | Rename as |
|----------------|------------------------------|-----------|
| -2             | No consumption               | NC        |
| -1             | Paid in full                 | PF        |
| 0              | The use of revolving credit  | URC       |
| 1 to 8         | Payment delay for 'X' months | PD'X'     |
Table: Codification of *PAY* categorical values

## Check distribution of Data  
Let's check the distribution of all the variables. For the continuous ones we can plot an histogram, for the categorical ones, a barplot with the distribution within the levels of the variable.
```{r ,echo=FALSE,include=FALSE,fig.cap="Variable distribution"}
knitr::opts_chunk$set(cache=TRUE)
save.plot(grid.plot(credit,15),"Variable_Distribution.jpeg","jpeg",plotDir,"1500","1500","110")
setwd(codeDir)
```
![Variable distribution](`r plotDir`/Variable_Distribution.png)
with this plot, we can see that the continuous data is very skewed, and not normal at all, we will apply some transformations to make the data more "normal"

### Check Zero variance predictors 
Are there any zero variance predictors? nearZeroVar() diagnoses predictors that have one unique value (i.e. are zero variance predictors) or predictors that are have both of the following characteristics: they have very few unique values relative to the number of samples and the ratio of the frequency of the most common value to the frequency of the second most common value is large. 
```{r Check Zero Values, echo=FALSE,include=FALSE}
 
library("caret")
cl <- makeCluster(detectCores())
registerDoParallel(cl)
zero.variance <- nearZeroVar(credit, saveMetrics = TRUE)
stopCluster(cl)
str(zero.variance)
zero.variance[zero.variance[,"zeroVar"] > 0, ] 
zero.variance[zero.variance[,"zeroVar"] + zero.variance[,"nzv"] > 0, ] 
```

There are none, we can conclude that all the predictors are relevant for the moment.

### Check missing Values 
TODO: document the observations and criteria to exclude the observations

```{r Check missing values, echo=FALSE,include=FALSE}
# First check N/A values
which(is.na(credit),arr.ind=TRUE) 
md.pattern(credit)
# It can't find any value expressed as 'NA', but there are some rows where all the values for the billing
# statements and the previous payment are 0, this could be treated as a missing value, because if there is 
# a credit card issued, there must be values for this columns, so we treat them as missing values. First 
# a check of how many of this occurrences exist is needed
check.new.clients<-function(input.data){
  indexes<-NULL
  j<-1
  for(i in 1:dim(input.data)[1]){
    if((!all(input.data[i,c(1:6)]=="NC")) && all(input.data[i,c(7:dim(input.data)[2])]==0)){
      indexes[j]<- i
      j<-j+1
    }
  }
  return(indexes)
}

check.zero.rows<-function(input.data){
  indexes<-NULL
  j<-1
  for(i in 1:dim(input.data)[1]){
    if((all(input.data[i,c(1:6)]=="NC")) && all(input.data[i,c(7:dim(input.data)[2])]==0)){
      indexes[j]<- i
      j<-j+1
    }
  }
  return(indexes)
}

# cl <- makeCluster(detectCores())
# registerDoParallel(cl)
num.new.clients<-check.new.clients(credit[,c(6:23)])
num.zeros.index<-check.zero.rows(credit[,c(6:23)])

# stopCluster(cl)
```

```{r Check missing values output, echo=FALSE,comment=NA}
#number of "inactive" users
length(num.zeros.index)
round((length(num.zeros.index)*100)/dim(credit)[1],2)

#number of "new" users
length(num.new.clients)
round((length(num.new.clients)*100)/dim(credit)[1],2)

credit<-credit[-num.zeros.index,]

# update continuous and factor data
credit.continuos<-credit[,-factor.indexes]
credit.factors<-credit[,factor.indexes]

```
|    | Total | Total(%) | 
|--------|--------|---------|
| Zero values found | `r length(num.zeros.index)` | `r round((length(num.zeros.index)*100)/dim(credit)[1],2)`  |
Table: Total count of values found as Zero 

|    | Total | Total(%) | 
|--------|--------|---------|
| Zero values found | `r length(num.new.clients)` | `r round((length(num.new.clients)*100)/dim(credit)[1],2)`  |
Table: Total count of values found as new clients

So in total there are 495 of this kind of data, it represents 1.68% of the data, but we will discard them anyway because this data can produce wrong calculations.

## Outlier Detection   

```{r Local Oulier Factor, echo=FALSE,fig.cap="Mahalanobis outlier detection"}
#***************************************************************************#
#                            2.4 Outlier Detection                          #
#***************************************************************************#
knitr::opts_chunk$set(cache=TRUE)
# #**************************** Outlier detection with lofactor (Local Outlier Factor) ***********************************
# #outlier detection with lofactor (Local Outlier Factor), takes a while 
# require(DMwR)
# outlier.scores <- lofactor(credit[,-factor.indexes], k=10)
# #we cannot plot, there are NaN, infinite values, possible cause is to have more repeated values than neighbours k
# plot(density(outlier.scores))
# #pick top 5 as outliers
# outliers <- order(outlier.scores, decreasing=T)[1:5]
# hist(outliers)
# #Which are the outliers?
# print(outliers)
# # we create a table of scores and id, to remove the supossed outliers
# scores <- cbind.data.frame(score = outlier.scores,id = rownames(credit.continuos))
# credit.lofactor <- credit[-as.numeric(scores[scores$score >= scores[outliers[5],]$score,]$id)]
# ```
# 
# ```{r, fig2,fig.width=8,fig.height=4,message=FALSE,fig.cap="Caption", echo=FALSE}

#**************************** Outlier detection Mahalanobis ***********************************
#outlier detection with mahalanobis 
require(chemometrics)
outlier.scores <- 
    Moutlier(credit[,-factor.indexes],quantile=0.975,plot=TRUE)
credit <- subset(credit,outlier.scores$md<outlier.scores$cutoff)

# update of continuous and categorical subsets
credit.continuos<-credit[,-factor.indexes]
credit.factors<-credit[,factor.indexes]

#cleaning up the house
remove(outlier.scores)

save.plot(Moutlier(credit[,-factor.indexes],quantile=0.975,plot=TRUE),"Mahalanobis_outlier_detection.jpeg","jpeg",plotDir,"1500","900","150")

# Altought, from all this, in some cases, they're could be just rich people in some variables, 
# or really indebted people in others
```


## Detection of most correlated continuous variables 

```{r, echo=FALSE,fig.cap="Correlation plot"}
invisible(require(corrplot))
par(mfrow=c(1,2))
corrplot(cor(credit[,-factor.indexes]), method="circle")
corrplot(cor(credit[,-factor.indexes]), method="number")
par(mfrow=c(1,1))
```
From the correlation calculus, we can see that there is a clear relationship between the values of BILL_AMT(x) and BILL_AMT(x+1), so we will use this variables for the PCA, using the other categorical and continuous variables as a supplementary one to observe if there is a dimension that it is not obviuos to capture.

## Detection of (correlation) of categorical variables 
In this part we are going to analyze each one of the categorial variables with respect to the response variable, if there is anything interesting that gives us more information.

### Sex 
```{r, echo=FALSE,comment=NA,fig.cap="Y - Sex relationship"}
par(mfrow=c(1,2))
# How many males and females do we have?
initial.barplot(credit,SEX)
# We have 34 % more females than males in our dataset.

# How many Default's and Not-Defaults's do we have for each sex?
grouped.count.plot(credit,SEX,default.payment.next.month)

# It seems that females tend to have less default payments, 
# lets compute the exact proportion to see if there is some kind of bias.
freq.table <- (with(data = credit, table(SEX, default.payment.next.month)))
p.table <- round(prop.table(freq.table, margin = 1), digits = 3)
cbind(freq.table, p.table)

create.latex.table(df=cbind(freq.table, p.table),type="latex",caption="Y and Sex frequencies",file=glue(dataDir,"/y_sex.tex"),digits = 0)
# As we see, the proportion of males with default payment is 0.242, and the proportion of females is 0.208.
# Indeed, males in general have a higher tendency of default payment.
```

### Education

```{r, echo=FALSE,comment=NA,fig.cap="Y - Education relationship"}
par(mfrow=c(1,2))
# a count of all the values to get an initial idea
initial.barplot(credit,EDUCATION)
# we can see that the 4 "unknown" values are very few, comparing them with the others
# also university is most present in this data, so a better way to see this is to group them all.

# a count check of all the education respect to default payment
grouped.count.plot(credit,EDUCATION,default.payment.next.month)
# university has the most population in both cases, but the tendency is to be not default,
# so a prior assumption will be that university level koreans will be unable to fill their
# debt obligations on time

# Again a check of the proportions will be useful
freq.table <- table(credit$EDUCATION, credit$default.payment.next.month)
p.table <- round(prop.table(freq.table, margin = 1), digits = 3)
cbind(freq.table, p.table)

create.latex.table(df=cbind(freq.table, p.table),type="latex",caption="Y and Education frequencies",file=glue(dataDir,"/y_education.tex"),digits = 0)

# The data have interesting results, first showing that not default are the most likely case in
# the unknown categories, and also showing that even if university is the bigest tendency,
# graduate level koreans are the ones that in proportion tend to be unable to fullfill their
# debt obligations in time
```

### Marriage

```{r, echo=FALSE,comment=NA,fig.cap="Y - Marriage relationship"}
par(mfrow=c(1,2))
# How are the levels of the variable distributed?
initial.barplot(credit,MARRIAGE)

# Basically we have 'Married' and 'Single' individuals, here we have the percentages of each type
round(prop.table(table(credit$MARRIAGE)) * 100, digits = 1)

# How many Default's and Not-Defaults's do we have for each type of marriage?
grouped.count.plot(credit,MARRIAGE,default.payment.next.month)

# Let's compute the exact proportion for each level to see if there is some kind of bias.
freq.table <- (with(data = credit, table(MARRIAGE, default.payment.next.month)))
p.table <- round(prop.table(freq.table, margin = 1), digits = 3)
cbind(freq.table, p.table)

create.latex.table(df=cbind(freq.table, p.table),type="latex",caption="Y and Marriage frequencies",file=glue(dataDir,"/y_marriage.tex"),digits = 0)

# As we can see, the proportion of 'Married' individuals with default payement is 0.235, while the proportion of 
# 'Single' is 0.209. 'Married' individuals have a higher tendency of default payement. 'Other' has a very low percentage 
# of default payment, but we just have 54 individuals, which is not enough data. 'Divorced' has the higher 
# percentage of default, but again we just have 323 individuals, compared to the +20000 rows that 
# are either 'Married' or 'Single'.
```

## Age

```{r, echo=FALSE,comment=NA,fig.cap="Y - Age relationship"}
par(mfrow=c(1,2))
# Even AGE is not categorical, we wanted to do an analysis to check how the age are related to the 
# default or not default category
# a count of all the values to get an initial idea
initial.barplot(credit,AGE)

grouped.count.plot(credit,AGE,default.payment.next.month)

invisible(require(plyr))
head(arrange(as.data.frame(table(credit$AGE)),desc(Freq)), n = 5)
# from this analysis, it is obvious that the quantity of users of credit cards, are centered around
# 29 years old
# Again a check of the proportions will be useful
freq.table <- table(credit$AGE, credit$default.payment.next.month)
p.table <- round(prop.table(freq.table, margin = 1), digits = 3)
(age.df<-as.data.frame(cbind(age=row.names(p.table),freq.table, p.table)))

head(arrange(age.df,desc(age.df$`Not default`)), n = 5)
# in this case, the proportions for not default are around 31 years, somewhat closed from the total
# around 29, but for 57 years, there is also a higher ratio in here, the default parameter is dominant
# in all the cases
head(arrange(age.df,desc(age.df$Default)), n = 5)
# this ratio is somewhat scattered, but they are around 51 and 64 years old, the most default is 
# predominant.

create.latex.table(df=head(arrange(age.df,desc(age.df$`Not default`)), n = 5),type="latex",caption="Y and Age Top 5 Not Default frequencies",file=glue(dataDir,"/y_age_not_default.tex"),digits = 0)

create.latex.table(df=head(arrange(age.df,desc(age.df$Default)), n = 5),type="latex",caption="Y and Age Top 5 Default frequencies",file=glue(dataDir,"/y_age_default.tex"),digits = 0)
```

## PCA Construction

Based on our correlation matrix, we decided to take the BILL variables as the explanatory ones in PCA, in an initial exploratory PCA, 

```{r, echo=FALSE}
n<-nrow(credit.continuos)
p<-ncol(credit.continuos)

#define matrix N
weight <- rep(1,n)
N <- diag(weight/sum(weight))

#calculate centroids
G<-t(as.matrix(credit.continuos)) %*% N %*% rep(1,n)

#centering X
X <-scale(credit.continuos,scale = FALSE,center = G)

#covariance matrix
covX <- t(X) %*% N %*% X

#standardizing
Xs<-scale(X)
Xs <- Xs * sqrt(n)/sqrt(n-1)

#get eigenvalues 
eigX <- eigen(covX)

save.plot(plot = plot(eigX$value,type="l",main="Screeplot"),name = "Screeplot.jpeg",type = "jpeg",plotDir = plotDir,height = "600",width = "700",res = "150" )
plot(eigX$value,ylab="Eigenvalues",type="l",main="Screeplot")
cumsum(100*eigX$values/sum(eigX$values))
require(FactoMineR)
e_ncp<-estim_ncp(X, ncp.min=0, ncp.max=p-1, scale=TRUE, method="GCV")
ncp<-e_ncp$ncp

#cleaning up the house
rm(N,G,X,Xs,covX)
gc()
```


```{r, echo=FALSE}
bill.indexes<-grepl("PAY_AMT", names(credit))
credit.PCA <- PCA(credit,quali.sup = factor.indexes,quanti.sup = c(18:23),ncp = ncp)
summary(credit.PCA)
```

```{r, echo=FALSE,fig.cap="PCA Colored according to cos2"}
require(factoextra)
#PCA Colored according to the quality of representation of the variables
fviz_pca_var(credit.PCA, col.var="cos2") +
scale_color_gradient2(low="white", mid="blue", high="red", midpoint=0.5) + theme_minimal()

fviz_pca_var(credit.PCA, select.var = list(contrib = 3))
# If a variable is perfectly represented by only two components, the sum of the cos2 is equal to one. In this case the variables will be positioned on the circle of correlations.
# For some of the variables, more than 2 components are required to perfectly represent the data. In this case the variables are positioned inside the circle of correlations.
```

```{r, echo=FALSE,fig.cap="PCA individuals Colored according to cos2"}
#Plot the quality of the representation for individuals on the principal components and colour them using the cos2 values
fviz_pca_ind(credit.PCA, col.ind="cos2") +
scale_color_gradient2(low="white", mid="blue", 
   high="red", midpoint=0.50) + theme_minimal()
```

```{r, echo=FALSE}
# Coordinates of variables
#loadings
head(credit.PCA$var$coord)
head(credit.PCA$var$contrib)
credit.PCA$eig
save.plot(plot = plot(credit.PCA$eig$eigenvalue,type="l",main="Screeplot PCA"),name = "Screeplot.jpeg",type = "jpeg",plotDir = plotDir,height = "600",width = "600",res = "150" )
plot(credit.PCA$eig$eigenvalue,type="l",main="Screeplot")
cumsum(100*credit.PCA$eig$eigenvalue/sum(credit.PCA$eig$eigenvalue))
e_ncp<-estim_ncp(credit.continuos, ncp.min=0, ncp.max=p-1, scale=TRUE, method="GCV")
nd = 3  #nd contains the number of relevant dimensions taken

#All the BILL_AMT are well represented and contribute the most to dim 1. LIMIT_BALL is more related to dim2
dimdesc(credit.PCA)
```

### MCA Construction

```{r, echo=FALSE}
credit.MCA<-MCA(credit, quali.sup = 24, quanti.sup = c(1,5,12:23) ,level.ventil = 0.2,ncp=20)
```

```{r, echo=FALSE}
#dimdesc(credit.MCA, axes=1:2, proba=0.05)
(desc.credit.mca<-dimdesc(credit.MCA))
```

```{r, include=FALSE}
setwd(dataDir)
for(i in 1:length(desc.credit.mca)){
  dim.name<-c("quanti","quali","category")
  # just take top 5 and bottom 5 of category
  eval(parse(text = glue("category<-rbind(head(as.data.frame(desc.credit.mca$`Dim ",i,"`$category), n = 5),tail(as.data.frame(desc.credit.mca$`Dim ",i,"`$category), n = 5))")))
  eval(parse(text = glue("desc.df<-cbindPad(as.data.frame(desc.credit.mca$`Dim ",i,"`$quanti),as.data.frame(rownames(desc.credit.mca$`Dim ",i,"`$quali)),as.data.frame(desc.credit.mca$`Dim ",i,"`$quali),as.data.frame(rownames(category)),category)")))
  eval(parse(text = glue("create.latex.table(df=desc.df,type=\"latex\",caption=\"Dimension ",i," description of dimensions\",file=\"dim",i,"_dimdesc_mca.tex\",digits = -2)")))
}
setwd(codeDir)

# Visualize top 10 variable categories
fviz_mca_var(credit.MCA, select.var = list(cos2 = 10))
```


### Cluster Definition

```{r, include=TRUE}
#*****************************************************************************************#
#   Definition of clusters   (Clustering)                                 #
#*****************************************************************************************#
#Generate the hierarchical structure. We can see 5 clusters in the first insight.
Psi = credit.PCA$ind$coord[,1:nd]
hc = hclust(dist(Psi), method = "ward.D2")
plot(hc)
abline(h=120, col="red")
#rect.hclust(hc, k=5, border = "red")
barplot(hc$height[(nrow(Psi)-20):(nrow(Psi)-1)])

#Cut the tree with the clusters defined. Then we generate the centers for the k-means and we do that.
nc = 5
cutT = cutree(hc, nc)
cdg = aggregate(as.data.frame(Psi),list(cutT),mean)[,2:(ncp+1)]
km = kmeans(Psi, centers = cdg)

#Plot k-means results. We can distinguish 5 clusters easily.
plot(Psi[,1],Psi[,2],main=glue("Clustering in ",nd, " classes"),col=km$cluster)
abline(h=0,v=0,col="gray")
text(Psi[,1],Psi[,2], col=km$cluster, labels=iden, cex = 0.6)
legend("topright",c("c1","c2","c3","c4","c5"),pch=20,col=c(1:5))

#Values assigned to each class generated
catdes(cbind(as.factor(km$cluster), credit.stratified), num.var = 1, proba = 0.05, row.w = NULL)

# simplied method that requiers more computational power, we need to do a sampling before use it
# require(NbClust)
# nb <- NbClust(credit[,-factor.indexes], distance = "euclidean", min.nc = 2,max.nc = 10, method = "complete", index ="all")
```

```{r, echo=FALSE}
credit.cluster.tagged<-cbind(credit,as.factor(km$cluster))
names(credit.cluster.tagged)[25]<-"cluster"
for(i in 1:5){
  eval(parse(text = glue("ind.clus.",i,"<-which(credit.cluster.tagged$cluster==",i,")")))
  eval(parse(text = glue("k",i,".table<-table(credit.cluster.tagged$default.payment.next.month[ind.clus.",i,"])")))
  eval(parse(text = glue("k",i,".table.freq<-prop.table(k",i,".table)")))
  eval(parse(text = glue("cbind(k",i,".table,k",i,".table.freq)")))
  eval(parse(text = glue("create.latex.table(df=cbind(k",i,".table,k",i,".table.freq),type=\"latex\",caption=\"Cluster ",i," frequencies\",file=glue(dataDir,\"/k",i,"_frequency.tex\",digits = 0))")))
}
```

We have generated 5 classes according the inspection that we have done in the data. Let's talk about each class and what values should have the individual to pertain in this class. The classes are labeled in the same order that in the previous plot:

 !!!!!  c1  !!!!! 
** Categorical Variables**
  - 73% of individuals are single 25% are married.
  - 53% have as Education="Univ." and 32% have the "Grad.". 13% come from "H. School".
  - Around 51-58% of individuals have values of PAY0, PAY_2, PAY_3, PAY_4, PAY_5 and PAY_6 equal to URC.
    Around 16-18% of individuals have values of PAY0, PAY_2, PAY_3, PAY_4, PAY_5 and PAY_6 equal to PF.
    
  - 74% of individuals have "Not Default" in the "default.payment.next.month". 26% are in "Default".

** Continuous Variables**
  - PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5 and PAY_AMT6 have quite lower values than the overall mean.
  - BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5 and BILL_AMT6 have quite lower values than the overall mean.
  - LIMIT_BAL has a quite lower values than the overall mean.
  - AGE with small lower values than the overall mean.
  

!!!! c2 !!!!! 
** Categorical Variables**
  - 46% of individuals are single 53% are married.
  - 53% have as Education="Grad"" and 35% have the "Univ". 10% from "H. School".
  - 35% of individuals are males. So, 65% will be females. 
  
  - Around 38-43% of individuals have values of PAY0, PAY_2, PAY_3, PAY_4, PAY_5 and PAY_6 equal to PF.
    Around 26-33% of individuals have values of PAY0, PAY_2, PAY_3, PAY_4, PAY_5 and PAY_6 equal to NC.
    Around 19-26% of individuals have values of PAY0, PAY_2, PAY_3, PAY_4, PAY_5 and PAY_6 equal to "URC".

  - 85% of individuals have "Not Default" in the "default.payment.next.month". 15% are in "Default".


** Continuous Variables**
  - PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5 and PAY_AMT6 have a bit higher values than the overall mean.
  - BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5 and BILL_AMT6 have quite lower values than the overall mean.
  - LIMIT_BAL has quite higher values than the overall mean.
  - AGE have similar values with the overall mean.


!!!!! c3  !!!!! 
It seems the most difficult class to classify.
** Categorical Variables**
  - 27% of individuals are single 68% are married.
  - 15% have as Education="Grad"". 35% from "H. School".
  
  - 45% of individuals are males. 55% of individuals are females.  
  
  - Around 11-18% of individuals have values of PAY0, PAY_1, PAY_2, PAY_3, PAY_4  equal to PD_2. 
          However, PAY_5 has only 1% with PD2.
      
    Around 10-12% of individuals have values of PAY_0, PAY_5 and PAY_6 equal to PD1.
    Around 18% of individuals have values of PAY_0, PAY_2 and PAY_3 equal to PF.
    
    Around 9%-12% of individuals have values of PAY_3, PAY_4, PAY_5 and PAY_6 equal to NC.
          However, PAY_0 and PAY_2 have around 6-8% with NC.

  - 72% of individuals have "Not Default" in the "default.payment.next.month". 28% are in "Default".


** Continuous Variables**
  - PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5 and PAY_AMT6 have quite lower values than the overall mean.
  - BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5 and BILL_AMT6 have quite lower values than the overall mean.
  - LIMIT_BAL has quite lower values than the overall mean.
  - AGE has with a bit higher values than the overall mean.

!!!!! c4  !!!!! 
** Categorical Variables**
  - 52% have as Education="Univ"". 15% from "H. School". 30% with "Grad."
  - 40% of individuals are males. 60% of individuals are females.  
  - Around 75-85% of individuals have values of PAY_0, PAY_2, PAY_3, PAY_4, PAY_5 and PAY_6  equal to "URC". 
    Around 14-16% of individuals have values of PAY0, PAY_2, PAY_3 and PAY_4  equal to "PD2". 
    Around 12-13% of individuals have values of PAY_5 and PAY_6  equal to "PD1". 


** Continuous Variables**
  - PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5 and PAY_AMT6 with higher values than the overall mean.
  - BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5 and BILL_AMT6 quite higher values than the overall mean.
  - LIMIT_BAL with a bit higher values than the overall mean.
  - AGE has similar values than the overall mean.

!!!!! c5  !!!!! 
** Categorical Variables**
  - 13% from "H. School". 38% with "Grad."
  - 49.5% of individuals are single 49.5% are married.  

  - Around 78-88% of individuals have values of PAY_0, PAY_1, PAY_2, PAY_3, PAY_4, PAY_5 and PAY_6  equal to "URC". 

** Continuous Variables**
  - PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5 and PAY_AMT6 have very higher values than the overall mean.
  - BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5 and BILL_AMT6 very higher values than the overall mean.
  - LIMIT_BAL with very higher values than the overall mean.
  - AGE has similar values than the overall mean.



## Construction of models
This dataset involves a prediction problem. The multivariate analysis we have done so far had helped us to identify concepts that were not so obvious. In this part we will construct Random forest and a Desicion tree predictive models, then check and compare their predictive values. 
In this exploratory analyzis, we first check the balance of the sample with respect to our response variable:
```{r, echo=FALSE,eval=TRUE,comment=NA}
#*****************************************************************************************#
#                              Initial model assumptions                                  #
#*****************************************************************************************#
# We first check if the test and train samples are balanced
rbind(noquote(table(credit$default.payment.next.month)),sapply(prop.table(table(credit$default.payment.next.month))*100, function(u) noquote(sprintf('%.2f%%',u))))
```
As we can see, the distribution of the data is unequal, if we take train and test samples as it is, we could get the model to ignore one of the classes (in this case the Default one, as it is the smallest), so we need to solve this in a simple way, we will take at random the same number of Not Default samples as the Default ones. Once this is done, we can split this subset into train and test data.
We are interested in check what is the importance of each predictor over the tree, for this we will use Boruta package, that finds relevant features by comparing the original attributes importance with importance achievable at random, estimated using their own permuted copies. 
The dataset contains 26994 observations, this will take a long time for Boruta to compute the values, so a stratified sample will be created, taking 10% of the total data and all the groups from *default.payment.next.month*, because this is our predictive variable. Let's check the distribution of the variables after the stratification sampling:
```{r Variable importance - stratified sample, echo=FALSE}
invisible(require(Boruta))
# first to speed up things, we take an 20% stratified random sample, from all the groups in default.payment.next.month
credit.stratified<-stratified(credit, "default.payment.next.month", 0.2)
#checking the distribution values
rbind(noquote(table(credit.stratified$default.payment.next.month)),sapply(prop.table(table(credit.stratified$default.payment.next.month))*100, function(u) noquote(sprintf('%.2f%%',u))))
```
So we confirm that indeed it is the 20% of the data, exactly what we wanted. Let's proceed with Boruta.
```{r Variable importance, echo=FALSE}
cl <- makeCluster(detectCores())
registerDoParallel(cl)
credit.importance <- Boruta(default.payment.next.month~., data = credit.stratified, doTrace = 2)
stopCluster(cl)
print(credit.importance)
```

```{r Variable importance - Plot, echo=FALSE,fig.cap="Importance plot"}
#plot boruta data, from https://www.analyticsvidhya.com/blog/2016/03/select-important-variables-boruta-package/
plot(credit.importance, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(credit.importance$ImpHistory),function(i)
credit.importance$ImpHistory[is.finite(credit.importance$ImpHistory[,i]),i])
names(lz) <- colnames(credit.importance$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels), at = 1:ncol(credit.importance$ImpHistory), cex.axis = 0.8)
```

As mentioned before
```{r sampling balance and subsampling, echo=FALSE}
#remove sex and pay_0
credit.sp<-credit[,-c(2,6)]
set.seed(123)
nd.indexes<-sample(which(credit$default.payment.next.month == "Not default"),6081)
d.indexes<-which(credit$default.payment.next.month == "Default")
subsample.index<-sort(c(nd.indexes,d.indexes))

table(credit[subsample.index,]$default.payment.next.month)

credit.train <- sample(subsample.index, size = ceiling(length(subsample.index)*0.7))
credit.test <- subset(subsample.index, !(subsample.index %in% credit.train))
```

```{r initial linear model, echo=FALSE}

# Checking PCA
# credit.glm<-glm((as.numeric(default.payment.next.month)-1) ~ ., data=credit[credit.train,-c(2,3,4,6,7,8,9,10,11)])
# print(summary(credit.glm))

```


```{r Desicion tree definition, echo=FALSE}
invisible(require(rattle))
set.seed(555)
credit.tree <- rpart(default.payment.next.month ~ ., data=credit.sp[credit.train,], control=rpart.control(cp=0.01, xval=10))
#feature selection is done intrinseclt
#cp is the complexity parameter, is the alpha. Maximum is 0
#xval 10 fold cross validation 
#with this we get the maximal tree
save.plot(plot=fancyRpartPlot(credit.tree),name = "desicion_tree.jpeg",type = "jpeg",plotDir = plotDir,width = "700",height = "600",res = "110")
fancyRpartPlot(credit.tree)
printcp(credit.tree)
```

```{r Desicion tree - number of partitions, echo=FALSE}
credit.tree.leaf=subset(credit.tree$frame, var=="<leaf>",select=c(n,yval2))
num_leaf = row.names(credit.tree.leaf)
credit.tree.leaf=data.frame(credit.tree.leaf$n,credit.tree.leaf$yval2)  # concatenating the two outputs
names(credit.tree.leaf) = c("n_train","class_train","n1_train","n2_train","p1_train","p2_train","probnode_train")
row.names(credit.tree.leaf) = num_leaf
credit.tree.leaf=credit.tree.leaf[order(-credit.tree.leaf$p2_train),] # ordering by decreasing positive probabilities

credit.tree.leaf$cum_n1 <- cumsum(credit.tree.leaf$n1_train)/sum(credit.tree.leaf$n1_train)
credit.tree.leaf$cum_n2 <- cumsum(credit.tree.leaf$n2_train)/sum(credit.tree.leaf$n2_train)
credit.tree.leaf$dif_cum <- credit.tree.leaf$cum_n2 - credit.tree.leaf$cum_n1

create.latex.table(df=print(credit.tree.leaf),type = "latex",caption = "Leaves information",file = "leaves_information.tex",digits = -2)
print(credit.tree.leaf)
```

```{r Desicion tree - complexity parameter, echo=FALSE}
plot(credit.tree$cptable[,2],credit.tree$cptable[,3],type="l",xlab="size of the tree",ylab="Relative impurity",main="R(t)")
lines(credit.tree$cptable[,2],credit.tree$cptable[,4],col="blue")
legend("topright",c("R(T)training","R(T)cv"),col=c("black","blue"),lty=1)
#missclasification always decrease, tend to 0
credit.tree
#plot the complexity parameter
par(mar=c(3,3,4,3))
plotcp(credit.tree)
```

```{r Desicion tree - Prunning, echo=FALSE, fig.cap="Variable importance"}
#optimal one is the lowest one, shows sd, lets take it
credit.tree$cptable <- as.data.frame(credit.tree$cptable)
ind <- which.min(credit.tree$cptable$xerror)

xerr <- credit.tree$cptable$xerror[ind]
xstd <- credit.tree$cptable$xstd[ind]

i <- 1
while (credit.tree$cptable$xerror[i] > xerr+xstd) i = i+1

alfa <- credit.tree$cptable$CP[i]

# prune the tree
p1 <- prune(credit.tree,cp=alfa)
p1
save.plot(plot = barplot(p1$variable.importance,las=2,cex.names=0.8),name = "Variable importance.jpeg",type = "jpeg",plotDir = plotDir,width = "900",height = "600",res = "110")
barplot(p1$variable.importance, main="Variable importance",las=2,cex.names=0.8)
```

```{r Desicion tree - Obtained values, echo=FALSE}
pred.test <- predict(p1, newdata=credit.sp[credit.test,],type="class")

invisible(library(caret))
confusionMatrix(pred.test, credit.sp[credit.test,]$default.payment.next.month,mode = "prec_recall")
confusionMatrix(pred.test, credit.sp[credit.test,]$default.payment.next.month,mode = "prec_recall",positive = "Default")
```

```{r ROC computation, echo=FALSE}
threshold = 0.5
pred.test.df<-as.data.frame(predict(p1, newdata=credit.sp[credit.test,],type="prob"))

invisible(require(ROCR))
pred <- prediction(pred.test.df$`Not default`, credit.sp$default.payment.next.month[credit.test])
roc <- performance(pred,measure="tpr",x.measure="fpr")
plot(roc, main="ROC curve")
abline(0,1,col="blue")
library(pROC)
auc(roc(credit.sp[credit.test,]$default.payment.next.month, pred.test.df$`Not default`))
# Area under the curve: 0.7026
```

```{r Random forest, echo=FALSE}
library(randomForest)
credit.rf <- randomForest(default.payment.next.month ~ ., data=credit[credit.train,], mtry=3, importance=TRUE, xtest=credit[credit.test,-24], ytest=credit[credit.test,24], nodesize=50, maxnodes=14, keep.forest=TRUE)
summary(credit.rf)
credit.rf$confusion
importance(credit.rf)
print(credit.rf)
varImpPlot(credit.rf)

pred.test.rf <- predict(credit.rf, credit[credit.test,],type="class")

library(caret)
confusionMatrix(pred.test.rf, credit[credit.test,]$default.payment.next.month,mode = "prec_recall")
confusionMatrix(pred.test.rf, credit[credit.test,]$default.payment.next.month,mode = "prec_recall",positive = "Default")
```

## Conclusions
* Item 1
* Item 2
    + Item 2a
    + Item 2b
    
## Further Work

```{r generateBibliography, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
require("knitcitations")
cleanbib()
options("citation_format" = "pandoc")
read.bibtex(file = "bibliography.bib")
```
